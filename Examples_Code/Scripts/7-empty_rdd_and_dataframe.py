# -*- coding: utf-8 -*-
"""6-Empty RDD and dataframe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bXuCLMUYp-YMzZIX61gN0Gaqqen9dS7n
"""

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Spark').getOrCreate()

#Creates Empty RDD
emptyRDD = spark.sparkContext.emptyRDD() 
print(emptyRDD)

#Creates Empty RDD using parallelize
rdd2= spark.sparkContext.parallelize([]) 
print(rdd2)

#Create Schema
from pyspark.sql.types import StructType,StructField, StringType
schema = StructType([
  StructField('firstname', StringType(), True),
  StructField('middlename', StringType(), True),
  StructField('lastname', StringType(), True)
  ])

#Create empty DataFrame from empty RDD
df = spark.createDataFrame(emptyRDD,schema)
df.printSchema()

#Create empty DataFrame directly. 
df2 = spark.createDataFrame([], schema)
df2.printSchema()

#Create empty DatFrame with no schema (no columns)
df3 = spark.createDataFrame([], StructType([]))
df3.printSchema()

#Convert empty RDD to Dataframe
df1 = emptyRDD.toDF(schema)
df1.printSchema()

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Spark').getOrCreate()
dept = [("Finance",10),("Marketing",20),("Sales",30),("IT",40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name","dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema = deptColumns)
deptDF.printSchema() 
deptDF.show(truncate=False)

from pyspark.sql.types import StructType,StructField, StringType 
deptSchema = StructType([     
    StructField('dept_name', StringType(), True), 
    StructField('dept_id', StringType(), True) 
]) 

deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

import pyspark 
from pyspark.sql import SparkSession 
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate() 
data = [("James","","Smith","36636","M",60000),
        ("Michael","Rose","","40288","M",70000),
        ("Robert","","Williams","42114","",400000),
        ("Maria","Anne","Jones","39192","F",500000),
        ("Jen","Mary","Brown","","F",0)]

columns = ["first_name","middle_name","last_name","dob","gender","salary"]
pysparkDF = spark.createDataFrame(data = data, schema = columns)
pysparkDF.printSchema()
pysparkDF.show(truncate=False)

pandasDF = pysparkDF.toPandas()
print(pandasDF)

